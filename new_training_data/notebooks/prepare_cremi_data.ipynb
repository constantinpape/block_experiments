{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare CREMI Data\n",
    "\n",
    "In this notebook, we will project the CREMI ground-truth neuron segmentation to (watershed) fragments.\n",
    "These will be stored in the bigcat CREMI format, to enable proofreading.\n",
    "\n",
    "The specific fragments used in this notebook were generated with a different CNN\n",
    "than the base segmentation of the original CREMI ground-truth.\n",
    "Because since CREMI was created 2 - 3 years ago, the networks have improved due to more training data and advances in design.\n",
    "(Also I don't know where to find original CREMI network predictions and fragments).\n",
    "\n",
    "Prerequisites to run this notebook:\n",
    "Besides standard python packages, we will need vigra, nifty and z5py to run the code.\n",
    "The easiest way to install it is in a clean conda environment:\n",
    "\n",
    "`conda create -n fix-cremi -c conda-forge -c ilastik-forge -c cpape python=3.6  nifty z5py jupyter vigra h5py`\n",
    "\n",
    "`source activate fix-cremi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all necessary packages\n",
    "# this might throw a runtime error, that can be ignored \n",
    "import sys\n",
    "import numpy as np\n",
    "import h5py\n",
    "import z5py\n",
    "import vigra\n",
    "import nifty.graph.rag as nrag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create output file\n",
    "\n",
    "We assume the following input data:\n",
    "- HDF5 file with the (padded and realigned) cremi data, containing raw data ('/volumes/raw'), ground-truth segmentation ('/volumes/neuron_ids') and a mask ('/volumes/labels/mask') indicating the actually labeled region.\n",
    "- N5 file containing the affinity predictions. For the cremi test samples, these predictions can be found in `/groups/saalfeld/home/papec/Work/neurodata_hdd/cremi_warped/sample*.n5`, `predictions/affs_glia`.\n",
    "\n",
    "The notebook will produce a HDF5 file with raw data, fragments and look-up table from segments to \n",
    "projected groundtruth segments, to be ingested by bigcat.\n",
    "The fragments will be reduced to the relevant bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the bounding box around the parts of the ground-truth\n",
    "# that is actually labeled\n",
    "def find_bounding_box_and_offset(path, mask_key):\n",
    "    # load the mask that indicates labeled parts of the data\n",
    "    with h5py.File(path, 'r') as f:\n",
    "        mask = f[mask_key][:]\n",
    "\n",
    "    # find the coordinates that are in the mask\n",
    "    coordinates = np.where(mask == 1)\n",
    "    # extract min and max masked coordinates\n",
    "    min_coords = [np.min(coords) for coords in coordinates]\n",
    "    max_coords = [np.max(coords) for coords in coordinates]\n",
    "    \n",
    "    # construct the bounding box\n",
    "    bb = tuple(slice(minc, maxc + 1) for minc, maxc in zip(min_coords, max_coords))\n",
    "\n",
    "    # compute the offset in nanometer for the bigcat format\n",
    "    resolution = (40., 4., 4.)\n",
    "    offset = tuple(b.start * res for b, res in zip(bb, resolution))\n",
    "    return bb, offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifiy all relevant paths\n",
    "# for this example we use the mala training data,\n",
    "# change paths for the actual test data accordingly\n",
    "\n",
    "# the sample we will process\n",
    "sample = 'A'\n",
    "\n",
    "# path and key to the cremi h5 file with groundtruth, raw data and groundtruth mask\n",
    "path_gt = '/home/papec/mnt/papec/20170312_mala_v2/sample_%s.augmented.0.hdf' % sample\n",
    "key_gt = 'volumes/labels/neuron_ids'\n",
    "key_raw = 'volumes/raw'\n",
    "key_mask = 'volumes/labels/mask'\n",
    "\n",
    "# path and key to the n5 file with affinity predictions\n",
    "path_affs = '/home/papec/mnt/papec/20170312_mala_v2/affs/sample_%s_affs.n5' % sample\n",
    "key_affs = 'predictions_mala'\n",
    "\n",
    "# the path to the output h5 file\n",
    "path_out = '/home/papec/mnt/papec/sample%s_fix_test.h5' % sample\n",
    "key_fragments = 'volumes/labels/fragments'\n",
    "\n",
    "# get the relevant bounding box\n",
    "bb, offset = find_bounding_box_and_offset(path_gt, key_mask)\n",
    "print(\"Groundtruth bounding box for sample\", sample)\n",
    "print(bb)\n",
    "print(\"Corresponding to offset in nanomenters:\")\n",
    "print(offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the output dataset and copy the raw data\n",
    "def create_output_file(in_path, out_path, key):\n",
    "    # check if we have the raw data in our out file already\n",
    "    with h5py.File(out_path) as f:\n",
    "        have_raw = key in f\n",
    "    \n",
    "    # if we do, we don't need to do anything here\n",
    "    if have_raw:\n",
    "        return\n",
    "    \n",
    "    # otherwise, load the raw data and write it to the out file\n",
    "    with h5py.File(in_path) as f:\n",
    "        raw = f[key][:]\n",
    "    with h5py.File(out_path) as f:\n",
    "        ds = f.create_dataset(key, data=raw, compression='gzip')\n",
    "        ds.attrs['resolution'] = [40., 4., 4.]\n",
    "        ds.attrs['offset'] = [0., 0., 0.]\n",
    "        # bigcat format\n",
    "        f.attrs['file_format'] = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_output_file(path_gt, path_out, key_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make watershed fragments\n",
    "\n",
    "We produce fragments using a 2d watershed on the averaged in-plane affinity predictions. For seeds, we\n",
    "use local maxima of its distance transform.\n",
    "This will result in 2d fragments, that are appropriate for anisotropic data.\n",
    "\n",
    "To run the function below, please download the `cremi_tools` repository:\n",
    "https://github.com/constantinpape/cremi_tools and append it to the pythonpath."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to redo the watersheds \n",
    "# to use this function, we need the additional `cremi_tools` repository:\n",
    "\n",
    "# it can be simply added to the pythonpath\n",
    "def run_watersheds(affinity_path, affinity_key,\n",
    "                   mask_path, mask_key, bounding_box):\n",
    "    # change to path where cremi_tools is located\n",
    "    sys.path.append('/home/papec/Work/my_projects/cremi_tools')\n",
    "    import cremi_tools.segmentation as cseg\n",
    "    \n",
    "    # load the xy affinities, average them over the xy-channels\n",
    "    # (we assume that xy correspond to channels 1 and 2 !)\n",
    "    f = z5py.File(affinity_path)\n",
    "    affs = f[affinity_key][(slice(1,3),) + bounding_box]\n",
    "\n",
    "    # convert affinities from 8bit to float if necessary\n",
    "    if affs.dtype == np.dtype('uint8'):\n",
    "        affs = affs.astype('float32') / 255.\n",
    "    # invert and average affinities to obtain height map for the 2d watersheds\n",
    "    hmap = np.mean(1. - affs, axis=0)\n",
    "\n",
    "    # load the mask to make sure watersheds adhere to the ground-truth boundaries\n",
    "    with h5py.File(mask_path) as f_mask:\n",
    "        mask = f_mask[mask_key][bounding_box].astype('bool')\n",
    "\n",
    "    # run the distance transform watersheds in 2d\n",
    "    # with threshold 0.4, sigma for smoothing 1.6 and minimum fragment size 20\n",
    "    ws = cseg.DTWatershed(0.4, 1.6, size_filter=20, is_anisotropic=True)\n",
    "    fragments, _ = ws(hmap, mask)\n",
    "    return fragments.astype('uint64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and save the fragments if necessary, otherwise load them from file\n",
    "with h5py.File(path_out) as f:\n",
    "    have_fragments = key_fragments in f\n",
    "    if have_fragments:\n",
    "        # load fragments from file\n",
    "        print(\"Fragments are already present, will load from file\")\n",
    "        frags = f[key_fragments][:]\n",
    "    else:\n",
    "        print(\"Fragments are not present, will compute and save\")\n",
    "        # create fragments\n",
    "        frags = run_watersheds(path_affs, key_affs, \n",
    "                               path_gt, key_mask, bb)\n",
    "        # write fragments to file and save necessary attributes\n",
    "        print(\"Computation done, saving fragments to\", path_out, key_fragments)\n",
    "        ds = f.create_dataset(key_fragments, data=frags, compression='gzip')\n",
    "        ds.attrs['offset'] = offset\n",
    "        ds.attrs['resolution'] = [40., 4., 4.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a slice of raw data overlaid with the fragments (or any other segmentation)\n",
    "def plot_segmentation(raw_path, raw_key, fragments, bounding_box,\n",
    "                      slice_id=0, alpha=0.6):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from skimage import img_as_float, color\n",
    "    \n",
    "    # load the slice from the raw data\n",
    "    with h5py.File(raw_path) as f:\n",
    "        bb_im = (slice_id + bounding_box[0].start,) + bounding_box[1:]\n",
    "        raw_im = f[raw_key][bb_im]\n",
    "    \n",
    "    # relabel this slice to make our live easier\n",
    "    frag_im = fragments[slice_id].copy()\n",
    "    vigra.analysis.relabelConsecutive(frag_im, out=frag_im,\n",
    "                                      start_label=1, keep_zeros=True)\n",
    "    \n",
    "    # convert the grayscale raw data to rgb\n",
    "    im = img_as_float(raw_im)\n",
    "    img_color = np.dstack((im, im, im))\n",
    "    # create random colors for fragments\n",
    "    \n",
    "    n_fragments = int(frag_im.max()) + 1\n",
    "    random_colors = np.random.rand(n_fragments, 3)\n",
    "    \n",
    "    # create the color mask\n",
    "    color_mask = np.zeros_like(img_color)\n",
    "    # we skip 0 (ignore label)\n",
    "    for frag_id in range(1, n_fragments):\n",
    "        color_mask[frag_im == frag_id, :] = random_colors[frag_id]\n",
    "    \n",
    "    # convert raw and fragments to hsv images\n",
    "    im_hsv = color.rgb2hsv(img_color)\n",
    "    mask_hsv = color.rgb2hsv(color_mask)\n",
    "    \n",
    "    # replace hue and saturation of the raw data\n",
    "    # with that of color mask\n",
    "    im_hsv[..., 0] = mask_hsv[..., 0]\n",
    "    im_hsv[..., 1] = mask_hsv[..., 1] * alpha\n",
    "    \n",
    "    im_colored = color.hsv2rgb(im_hsv)\n",
    "    f, ax = plt.subplots(figsize=(12, 12))\n",
    "    ax.imshow(im_colored)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the fragments for one slice\n",
    "plot_segmentation(path_gt, key_raw, frags, bb, slice_id=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project ground-truth to fragments\n",
    "\n",
    "Finally, we project the ground-truth segmentation to the fragments\n",
    "we have just produced, via maximum overlap projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project the cremi groundtruth to the watershed fragments\n",
    "# TODO correct cremi-ignore label ?!\n",
    "def project_gt_to_fragments(path, gt_key, fragments,\n",
    "                            bounding_box,\n",
    "                            cremi_ignore=0xfffffffffffffffd):\n",
    "    # load the CREMI groundtruth from h5 \n",
    "    with h5py.File(path) as f:\n",
    "        gt = f[gt_key][bounding_box]\n",
    "    assert gt.shape == fragments.shape\n",
    "\n",
    "    # build region adjacency graph and us it to\n",
    "    # extract th assignment of groundtruth labels\n",
    "    # to fragments\n",
    "    rag = nrag.gridRag(fragments.astype('uint32'))\n",
    "    assignment = nrag.gridRagAccumulateLabels(rag, gt).astype('uint64')\n",
    "    \n",
    "    # assign ignore label to 0 and relabel the assignment ids\n",
    "    if 0 in assignment:\n",
    "        assignment += 1\n",
    "    assignment[0] = 0\n",
    "    vigra.analysis.relabelConsecutive(assignment, out=assignment,\n",
    "                                      start_label=1, keep_zeros=True)\n",
    "\n",
    "    # add the number of fragments as offset to the assignments, because\n",
    "    # in bigcat fragment ids are also segment ids\n",
    "    n_fragments = rag.numberOfNodes\n",
    "    assignment[1:] += n_fragments\n",
    "    # find the next valid segment id\n",
    "    next_id = int(assignment.max() + 1)\n",
    "\n",
    "    # build the correct lut format for bigcat (fragment ids are consecutive, starting at 0)\n",
    "    lut = np.array([(frag_id, seg_id)\n",
    "                    for frag_id, seg_id in enumerate(assignment)],\n",
    "                   dtype='uint64')\n",
    "    lut = lut.transpose()\n",
    "\n",
    "    # set the masked area to the cremi ignore value\n",
    "    lut[1, 0] = cremi_ignore\n",
    "\n",
    "    # map the assignments to a volumetric segmentation for inspection purposes\n",
    "    projected = nrag.projectScalarNodeDataToPixels(rag, assignment)\n",
    "    return lut, next_id, projected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the segment to fragment lut\n",
    "lut, next_id, projected = project_gt_to_fragments(path_gt, key_gt, frags, bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the projection for a slice\n",
    "plot_segmentation(path_gt, key_raw, projected, bb, slice_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize the lut and next-id\n",
    "with h5py.File(path_out) as f:\n",
    "    f.create_dataset('fragment_segment_lut', data=lut, chunks=True, maxshape=(2, None))\n",
    "    f.attrs['next_id'] = int(next_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
